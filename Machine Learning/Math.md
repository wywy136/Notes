# 数学基础

## 线代

### 范数

$L^p$范数：$||x||_p=(\sum_i|x_i|^p)^{\frac{1}{p}}$

p=2时为L2范数，由于很常用，经常简化表示为$||x||$

平方L2范数比L2范数计算更方便，例如求导时

### 特征分解

方阵A的特征向量指与A相乘后相当于对该向量进行缩放的非零向量v
$$
Av=\lambda v
$$
其中$\lambda$称为这个特征向量对应的特征值。特征向量被缩放后仍然是这个矩阵的特征向量，因此通常只考虑单位特征向量

矩阵A的特征分解可以记作
$$
A=Vdiag(\lambda)V^{-1}
$$
其中$V=[v_1,...,v_n]$中每个列向量v都是矩阵A的线性无关的特征向量，$\lambda=[\lambda_1,...,\lambda_n]$是特征向量$v_i$对应的特征值

每个实对称矩阵都可分解成实特征向量和实特征值：
$$
A=Q\Lambda Q^T
$$
其中Q是A的特征向量组成的正交矩阵，$\Lambda_{i,i}$对应的特征向量是矩阵Q的第i列

## 数值计算

### 基于梯度的优化方法

将x沿导数的反方向移动一小步来减小$f(x)$ - 梯度下降

对于多为输入的函数，梯度是相对一个向量求导的导数：f的梯度是包含所有偏导数的向量，记为$\nabla_xf(x)$，梯度的第i个元素是f关于xi的偏导数

对于输入和输出都为向量的函数，包含所有这样偏导数的矩阵称为Jacobian矩阵：$J_{i,j}=\frac{\part}{\part x_j}f(x)_i$

当函数具有多维输入，二阶导数也有很多，将这些导数合并为Hessian矩阵：$H(f)(x)_{i,j}=\frac{\part^2}{\part x_i\part x_j}f(x)$

