# 机器学习

- 西瓜书
- 《统计学习方法》
- 机器学习研究生课程

## 概论

监督学习的任务是学习一个模型，对给定的输入预测相应的输出

- 生成式方法：由数据学习联合概率分布$P(X,Y)$，之后求出产生输出Y的模型：$P(Y|X)=\frac{P(X,Y)}{P(X)}$ - 比如朴素贝叶斯和隐马尔科夫模型
  - 优点：可以还原出联合概率分布；收敛速度更快；存在隐变量时仍可使用
- 判别式方法：直接学习条件概率分布$P(Y|X)$作为预测 - k近邻、感知机、逻辑回归、最大熵模型、SVM、CRF、Boosting
  - 优点：直接面对预测，学习的准确率更高；可以对数据进行抽象、定义特征、使用特征

## 模型评估

### ROC曲线与AUC

1. ROC横轴：假正例率（$FPR=\frac{FP}{FP+TN}$），纵轴：真正例率（$TPR=\frac{TP}{TP+FN}$）
2. AUC是ROC与坐标轴包住的面积，AUC越大模型性能越好

### 交叉验证

- 简单交叉验证：70%训练，30%测试
- S折交叉验证：S个互不相交的子集，S-1训练，1测试，从S个中选出最好的

## 线性模型

$f(x)=w^Tx+b,w=(w_1,w_2,...,w_d)$

线性模型可解释性好

### 线性回归

是一个回归模型，给$x$预测$y$

假设属性数目只有一个；数据集包含m个样本；最小化均方误差：$(w^*,b^*)=\arg\min\sum^m_{i=1}(y_i-wx_i-b)^2$

线性回归最小二乘“参数估计”：求解w和b使$E_{(w,b)}=\sum^m_{i=1}(y_i-wx_i-b)^2$最小化

将E对w，b求偏导，因为E是w和b的凸函数，所以偏导=0时，得到w和b的最优解

多属性对应多元线性回归：$E_{\hat w}=(y-X\hat w)^T(y-Xw^T), \hat w=(w;b), X\in R^{m*(d+1)}$

将E对w尖求偏导，如果$X^TX$为满秩矩阵或正定矩阵时，其可逆，$\hat w^*=(X^TX)^{-1}X^Ty$，多元线性回归模型为$f(\hat x_i)=\hat x^T_i(X^TX)^{-1}X^Ty, \hat x_i=(x_i, 1)$

现实中样本属性值数量往往大于样本数量，导致$X^TX$不满秩，会解出多个$\hat w$，选择将由正则化项决定

对数线性回归，$\ln y=w^Tx+b$，即让$e^{w^Tx+b}$逼近y

### 对数几率回归（Logistic）

用于2分类：$y=\frac{1}{1+e^{-z}}$。带入$z=w^Tx+b$，得到$\ln \frac{y}{1-y}=w^Tx+b$，$\frac{y}{1-y}$表示几率，是x作为正例的相对可能性，因此称作“对数几率”回归

对数几率回归无需事先假设数据分布；不是仅预测出类别，而是可得到近似概率

将上式写成类后验概率估计形式
$$
p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\\
p(y=0|x)=\frac{1}{1+e^{w^Tx+b}}
$$
通过极大似然法估计w和b：$l(w,b)=\sum^m_{i=1}\ln p(y_i|x_i,w,b)$

令$\beta=(w;b),\hat x=(x;1), p_1(\hat x;\beta)=p(y=1|\hat x,\beta),p_0(\hat x;\beta)=p(y=0|\hat x,\beta)=1-p_1(\hat x,\beta)$

将似然项重写为：$p(y_i|x_i,w,b)=y_ip_1(\hat x;\beta)+(1-y_i)p_0(\hat x;\beta)$

==公式3.27的推导没太看懂==

### 线性判别分析LDA

学习一条直线，同类的投影点尽可能接近，异类的投影点尽可能远离

最大化异类样例的投影点中心距离；最小化同类样例投影点的协方差

定义类内散度矩阵Sb和类间散度矩阵Sw，LDA最大化Sb和Sw的广义瑞利商

$w=S^{-1}_w(\mu_0-\mu_1)$，可对Sw进行奇异值分解来保证数值解的稳定性

### 多分类学习

拆分为脱肛额二分类任务：

- 一对一策略：将N个类别两两配对，产生N(N-1)/2个二分类任务，最终投票
- 一对其余：将一个类的样例作为正例，其他类的样例作为反例，训练N个分类器
- 多对多策略：ECOC

### 类别不平衡问题

再缩放：修改预测机率为：用分类器的预测几率*实际观测几率的倒数 - 阈值移动

欠采样与过采样：过采样不能简单地对初始正例样本重采样，否则会导致过拟合；SMOTE通过插值来进行过采样；EsayEnsemble将反例划分为若干个集合供不同学习器使用，从全局看不会损失重要信息

## 决策树

### 划分属性的选择

样本集合纯度的度量：信息熵：假设当前样本集合D中第k类样本所占比例为$p_k$，则D的信息熵定义为：$Ent(D)=-\sum^{|Y|}_{k=1}p_k\log_2p_k$，Y为类别数量。信息熵越小说明纯度越高

信息增益：$Gain(D,a)=Ent(D)-\sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)$，V是指当前属性可能取值的数量，$D^v$是对应取值的样本集合。信息熵和条件熵之差一般称为互信息

- 信息增益越大，说明用这个属性进行划分的纯度提升越大

信息增益准则对可取值数目较多的属性有偏好，减小偏好的方法：使用“增益率”来选择划分属性，用于C4.5算法中：$Gain-ratio(D,a)=\frac{Gain(D,a)}{IV(a)},IV(a)=-\sum^V_{v=1}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}$

- IV(a)称为属性a的固有值，a的可能取值数目越多，则IV(a)就越大

信息增益率准则对可取值数目较少的属性有偏好，因此C4.5算法结合两种方法：先从候选属性中找信息增益高于平均水平的，再从中选增益率最高的

CART决策树采用基尼指数作为选择依据：$Gini(D)=\sum^{|Y|}_{k=1}\sum_{k'\ne k}p_kp_{k'}$：Gini越小纯度越高

$Gini-index(D,a)=\sum^V_{v=1}\frac{|D^v|}{|D|}Gini(D^v)$，最小的Gini-index对应的属性应该被选择

### 剪枝

预剪枝：先评价划分当前节点能否带来泛化性能提升 - 避免过拟合、降低训练时间、但由于贪心策略可能导致欠拟合

后剪枝：先生成，后判断将当前非叶节点子树替换为叶节点能否带来泛化性能提升 - 比预剪枝保留更多的分支，防止欠拟合，但训练时间开销大很多

### 连续值与缺失值

二分法来处理连续变量：把属性值区间$[a^i,a^{(i+1)})$的中位点作为候选划分点，选取最优划分点：

$Gain(D,a)=\max_{t\in T_a}Gain(D,a,t)=\max_{t\in T_a}Ent(D)-\sum_{\lambda\in\{-,+\}}\frac{|D^\lambda_t|}{|D|}Ent(D^\lambda_t)$

与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性

如果属性a缺失，定义$\tilde D$为D中属性a不缺失的样本集合，$\tilde D_k$表示属于第k(1<=k<=Y)类的样本子集，$\tilde D^v$表示属性a上取值为$a^v(1<=v<=V)$的样本子集。为每个样本赋予权重$w_x$，定义
$$
\rho=\frac{\sum_{x\in\tilde D}w_x}{\sum_{x\in D}w_x}\\
\tilde p_k=\frac{\sum_{x\in\tilde D_k}w_x}{\sum_{x\in\tilde D}w_x}\\
\tilde r_v=\frac{\sum_{x\in\tilde D^v}w_x}{\sum_{x\in\tilde D}w_x}
$$
信息增益计算式推广为$Gain(D,a)=\rho*Gain(\tilde D,a)=\rho*(Ent(\tilde D)-\sum^V_{v=1}\tilde r_v Ent(\tilde D^v)),Ent(\tilde D)=-\sum^{Y}_{k=1}\tilde p_k\log_2\tilde p_k$

从而可以在a有缺失值的情况下选择合适的a

另外一个问题是若样本在划分属性上的值缺失，如何对样本进行划分？同时将样本划分到所有子节点中，同时将样本权值调整为$\tilde r_v*w_x$

##### 总结：决策树学习算法

![decisiontree](..\img\decisiontree.png)

## 神经网络

### 感知机

判别式线性分类模型：$f(x)=sign(wx+b)$

权重更新：$w_i\leftarrow w_i+\Delta w_i,\Delta w_i=\eta(y-y_i)x_i$

一层功能神经元只能求解线性可分的问题，不能解决异或这样的非线性可分问题

### 反向传播算法

BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整

sigmoid函数($f(x)=\frac{1}{1+e^{-x}}$)性质：$f'(x)=f(x)(1-f(x))$

标准BP算法推导：西瓜书P102-P104

标准BP算法（针对单个样例进行更新）和累计BP算法（直接对累计误差最小化）的区别类似于随机梯度下降和标准梯度下降的区别；很多任务中，累计误差下降到一定程度之后，进一步下降会非常缓慢，这时标准BP往往会较快地获得较好的解，在训练集非常大时更明显

防止过拟合的方法：

- 早停：验证集误差升高时停止
- 正则化：目标函数中加入描述网络复杂度的部分：$\sum_iw_i^2$

### 其他常见网络

- 径向基函数（RBF）网络：单隐层NN，使用径向基函数作为隐层神经元激活函数，输出层是对隐层神经元输出的线性组合：$\phi(x)=\sum^q_{i=1}w_i\rho(x,c_i)$
  $c_i$是第i个神经元对应的中心；$\rho(x,c_i)$径向基函数，定义为样本x到数据中心ci之间的欧氏距离单调函数，常用高斯径向基函数$\rho(x,c_i)=e^{-\beta_i||x-c_i||^2}$。确定ci常用随机采样和聚类的方法
- ART网络
- SOM网络
- Elman网络
- 玻尔兹曼机：最小化能量函数。神经元分为显层和隐层：显层表示数据的输入和输出，隐层表示数据的内在状态。神经元都是bool型，1表示激活，0表示抑制。令$s\in\{0,1\}^n$表示n个神经元的状态，则其对应的玻尔兹曼机能量定义为$E(s)=-\sum^{n-1}_i\sum^n_{j=i+1}w_{ij}s_is_j-\sum^n_{i=1}\theta_is_i$
  状态向量s出现的概率：$P(s)=\frac{e^{-E(s)}}{\sum_te^{-E(t)}}$
  受限玻尔兹曼机仅保留显层与隐层之间的连接，采用“对比散度”算法进行训练

## 支持向量机

### 对偶问题

划分超平面：$w^Tx+b=0$，w为法向量，决定了超平面的方向，b决定了超平面与原点之间的距离，样本空间中任一点x到超平面(w,b)的距离：$r=\frac{|w^Tx+b|}{||w||}$

距离超平面最近的几个训练样本点使下式等号成立
$$
w^Tx_i+b>=+1,y_i=+1\\
w^Tx_i+b<=-1,y_i=-1
$$
这些训练样本点被称为“支持向量”，2个异类支持向量到超平面的距离之和：$\gamma=\frac{2}{||w||}$，被称为“间隔”

需要找到使间隔最大的w和b：
$$
\max_{w,b}\frac{2}{||w||},s.t.y_i(w^Tx_i+b)>=1,i=1,2,...,m
$$
等价于最小化$\frac{1}{2}||w||^2$。需要求解上述问题。

> 为什么要最大化间隔？认为支持向量定义了各自的边界

使用拉格朗日乘子法求解：$L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum^m_{i=1}\alpha_i(1-y_i(w^Tx_i+b))$

L对w，b分别求偏导=0，可得
$$
w=\sum^m_{i=1}\alpha_iy_ix_i\\
0=\sum^m_{i=1}\alpha_iy_i
$$
带入L，消去w和b，可得**对偶问题**：
$$
\max_\alpha\sum^m_{i=1}\alpha_i-\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_jx^T_ix_j\\
s.t.\sum^m_{i=1}a_iy_i=0\\
a_i>=0
$$
解出$\alpha$后求出w和b，即可得到模型$f(x)=\sum^m_{i=1}\alpha_iy_ix^T_ix$

由于原问题中存在不等式约束，上述过程需要满足KKT条件，即对任何样本总有$\alpha_i=0$或$y_if(x_i)=1$：
$$
\alpha_i>=0;\\
y_if(x_i)-1>=0\\
\alpha_i(y_if(x_i)-1)=0
$$
若$\alpha_i=0$，则样本$x_i$不会在模型求和中出现；若$y_if(x_i)=1$，说明这个样本点是一个支持向量，位于最大间隔边界 - SVM训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关

SMO算法用于求解对偶问题。SMO每次选择两个变量$\alpha_i,\alpha_j$，固定其他参数，求解对偶问题公式1获得更新后的$\alpha_i,\alpha_j $。SMO选取两个变量所对应的两个样本之间的间隔是最大的，可以给目标函数值带来更大的变化。

SMO算法高效性来自于固定其他参数后，带入约束条件，对偶问题公式1变成了单变量二次规划问题，不必调用数值优化算法即可高效地计算出更新后的$\alpha_i,\alpha_j$

求解b：$y_s(\sum_{i\in S}\alpha_iy_ix^T_ix_s+b)=1$，其中S为所有支持向量的下标集，再使用所有支持向量求解平均值：$b=\frac{1}{|S|}\sum_{s\in S}(\frac{1}{y_s}-\sum_{i\in S}\alpha_iy_ix^T_ix_s)$

##### 总结：线性可分SVM学习算法

- 输入：线性可分训练集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\},y_i\in\{+1,-1\},i=1...N$

- 构造并求解约束最优化问题（对偶问题）
  $$
  \max_\alpha\sum^N_{i=1}\alpha_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_jx^T_ix_j\\
  s.t.\sum^N_{i=1}a_iy_i=0\\
  a_i>=0
  $$
  利用SMO算法求得最优解$\alpha^*=(\alpha^*_1,\alpha^*_2,...,\alpha^*_N)^T$

- 求解$w^*$和$b^*$：
  $$
  w^*=\sum^N_{i=1}\alpha_i^*y_ix_i\\
  b^*=\frac{1}{N}\sum_{j=1}^N(\frac{1}{y_j}-\sum_{i=1}^N\alpha_i^*y_ix^T_ix_j)
  $$

- 求得分类决策函数：
  $$
  f(x)=sign(w^*x+b^*)
  $$


### 核函数

用于解决非线性SVM

$\phi(x)$将是x映射到高维空间之后的特征向量，因此在特征空间中模型表示为$f(x)=w^T\phi(x)+b$

类似地有SVM原始问题和对偶问题，但计算$\phi^T(x)\phi(x)$困难 - 设想下面这个用于计算高维空间向量内积的核函数：$k(x_i,x_j)=\langle \phi(x_i),\phi(x_j)\rangle=\phi^T(x_i)\phi(x_j)$ - 核技巧

核技巧：通过只定义核函数k，而不显式地定义映射函数$\phi(x)$，对于给定的核k，特征空间和映射函数$\phi(x)$的取法并不唯一

因此模型求解之后写为$f(x)=\sum^m_{i=1}\alpha_iy_ik(x,x_i)+b$，成为支持向量展开式

定理：令X为输入空间，$k(·,·)$是定义在X*X上的对称函数，则$k$是核函数当且仅当对于任意数据D={x1,...,xm}，核矩阵$K(K\in R^{m * m},K_{ij}=k(x_i,x_j))$总是半正定的

核函数对于SVM的性能至关重要，若选择不合适，导致样本被映射到一个不合适的特征空间。常用的核函数：

- 线性核：$k(x_i,x_j)=x^T_ix_j$
- 多项式核：$k(x_i,x_j)=(x^T_ix_j)^d$
- 高斯核：$k=\exp(-\frac{||x_i-x_j||^2}{2\sigma^2})$
- 拉姆拉斯核：$k=\exp(-\frac{||x_i-x_j||}{\sigma})$
- sigmoid核：$k=\tanh(\beta x^T_ix_j+\theta)$

### 软间隔和正则化

线性可分SVM算法不适用于线性不可分的数据

软件线性不可分与过拟合问题：允许SVM在一些样本上出错，优化目标写为：$\min_{w,b}\frac{1}{2}||w||^2+C\sum^m_{i=1}l_{0/1}(y_i(w^Tx_i+b)-1)$，C>0为常数，l是0/1损失函数。若C无穷大，等价于标准问题；若C有限，允许一些样本不满足硬间隔要求

常见的替代损失函数

- hinge损失：$\max(0,1-z)$
- 指数损失：$\exp(-z)$
- 对率损失：$\log(1+\exp(-z))$

采用hinge损失，优化目标写为：$\min_{w,b,\xi_i}\frac{1}{2}||w||^2+C\sum^m_{i=1}\max(0,1-y_i(w^Tx_i+b))$，引入松弛变量$\xi_i>=0$，重写为
$$
\min_{w,b,\xi_i}\frac{1}{2}||w||^2+C\sum^m_{i=1}\xi_i\\
s.t.y_i(w^Tx_i+b)>=1-\xi_i\\
\xi_i>=0,i=1,2,...,m
$$
总之，如果样本符合硬间隔，那么优化目标就只包含w；如果是软间隔，还要加上这一样本的y(wx+b)。对每个样本，如果$\xi_i=0$，说明完全满足硬间隔约束

类似硬间隔，拉格朗日法求解得到对偶问题，唯一的区别在于$\alpha$的取值范围，在引入核函数后可得到同样的支持向量展开式

软间隔SVM的KKT条件：
$$
\alpha_i>=0,\mu_i>=0\\
y_if(x_i)-1+\xi_i>=0\\
\alpha_i(y_if(x_i)-1+\xi_i)=0\\
\xi_i>=0,\mu_i\xi_i=0
$$
对于任意训练样本，若$\alpha_i=0$，则样本不会对f(x)有任何影响；若$\alpha_i>0$，则该样本是**支持向量**。由拉格朗日函数求偏导可知，若$a_i<C$，则$\mu_i>0$，则$\xi_i=0$，说明样本恰好在最大间隔边界；若$\alpha_i=C$，则$\mu_i=0$，此时若$\xi_i<=1$则样本在分离超平面上或分离超平面与最大间隔之间；若$\xi_i>1$则此样本被错误分类（参见统计机器学习P130图7.5）

##### 总结：线性SVM学习算法

- 输入：线性可分训练集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\},y_i\in\{+1,-1\},i=1...N$

- 构造并求解约束最优化问题（对偶问题）
  $$
  \max_\alpha\sum^N_{i=1}\alpha_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_jx^T_ix_j\\
  s.t.\sum^N_{i=1}a_iy_i=0\\
  0<=a_i<=C
  $$
  利用SMO算法求得最优解$\alpha^*=(\alpha^*_1,\alpha^*_2,...,\alpha^*_N)^T$

- 求解$w^*$和$b^*$：
  $$
  w^*=\sum^N_{i=1}\alpha_i^*y_ix_i\\
  b^*=\frac{1}{N}\sum_{j=1}^N(\frac{1}{y_j}-\sum_{i=1}^N\alpha_i^*y_ix^T_ix_j)
  $$

- 求得分类决策函数：
  $$
  f(x)=sign(w^*x+b^*)
  $$


### SVR

允许预测值$f(x)$和真实值y之间有$\epsilon$的差别，形式化为:$\min_{w,b}+C\sum^m_{i=1}l_\epsilon(f(x_i)-y_i)$

推导略，见西瓜书P135-P136。SVR解形如：$f(x)=\sum^m_{i=1}(\hat \alpha_i-\alpha_i)x^T_ix_i+b$，使$(\hat \alpha_i-\alpha_i)$不为0的点是SVR的支持向量，仍然仅仅是训练样本的一部分。

核函数依然适用

### SMO算法

SMO算法每次选择2个变量，针对这两个变量构建一个二次规划问题，通过解析方法求解，提高计算速度。因此SMO算法包含两部分：求解2个变量二次规划的解析方法和选择变量的启发式方法。注意，这2个变量只有一个是自由变量，另一个变量取决于这个自由变量

**2个变量二次规划的求解方法**

假设选择的两个变量为$\alpha_1,\alpha_2$，则将SVM对偶问题重写为
$$
\min \ \ \ W(\alpha_1,\alpha_2)=\frac{1}{2}K_{11}\alpha^2_1+\frac{1}{2}K_{22}\alpha^2_2+y_1y_2K_{12}\alpha_1\alpha_2-(\alpha_1+\alpha_2)+y_1\alpha_1\sum^N_{i=3}y_i\alpha_iK_{i1}+y_2\alpha_2+\sum^N_{i=3}y_i\alpha_iK_{i2}\\
s.t.\ \ \ \ \ \alpha_1y_1+\alpha_2y_2=-\sum^N_{i=3}y_i\alpha_i\\
0<=\alpha_i<=C
$$
相当于在盒子$[0,C]\times[0,C]$的平行于对角线的直线上，找出使目标函数最小的值，实际上是单变量的最优化问题

利用约束1，将目标函数的$\alpha_1$消去：$\alpha_1y_1=\alpha_1^{old}y_1+\alpha^{old}_2y_2-\alpha_2y_2$，得到关于$\alpha_2$的函数，对其求导=0，再考虑上下限（对角线），得到$\alpha_2$，再根据这两个变量在一条直线上得到$\alpha_1$。具体结果见《统计学习方法》P145

**变量的选择方法**

第一个变量的选择：外层循环，选取违反KKT条件最严重的样本点对应的变量

第二个变量的选择：选择使$|E_1-E_2|$最大的$\alpha_2$，$E_i=g(x_i)-y_i=(\sum_j \alpha_jy_jK(x_j,x_i)+b)-y_i$

## 贝叶斯分类器

属于生成模型

### 贝叶斯决策论

假设N中可能的类别，$\lambda_{ij}$是将一个真是标记为$c_j$的样本误分类为$c_i$所产生的损失，基于后验概率获得将样本$x$分类为$c_i$所产生的条件风险：$R(c_i|x)=\sum^N_{j=1}\lambda_{ij}P(c_j|x)$，寻找一个判定准则$h:X->Y$以最小化总体风险
$$
R(h)=E_x[R(h(x)|x)]
$$
为最小化总体风险，只需在每个样本上（逐个）选择使条件风险最小的类别：$h^*(x)=\arg\min_{c\in Y}R(c|x)$，与之对应的风险$R(h*)$称为贝叶斯风险。

若$\lambda_{ij}=0(i=j)/1(else)$，则条件风险可写为$R(c|x)=1-P(c|x)$，则$h^*(x)=\arg\max_{c\in Y}P(c|x)$

获得后验概率$P(c|x)$的方式有2种：

- 判别式：直接建模$P(c|x)$，比如决策树、神经网络、SVM
- 生成式：$P(c|x)=\frac{P(x,c)}{P(x)}$。基于贝叶斯定理，$P(c|x)=\frac{P(c)P(x|c)}{P(x)}$

其中$P(x|c)$是类条件概率，也叫做“似然”。直接由训练样本估计P(x|c)因为训练数据的缺少性导致很多情况不被观测，不可取。

### 极大似然估计

先验概率$P(c)$的极大似然估计：
$$
P(c)=\frac{\sum^N_{i=1}I(y_i=c)}{N}
$$
假设类条件概率$P(x|c)$符合某种分布，被参数向量$\theta_c$唯一地确定，将似然率写作$P(x|\theta_c)$

概率模型的训练：参数估计的两种观点：

- 频率派：未知的参数是客观存在的固定值，可通过优化似然函数来确定 - 极大似然估计
- 贝叶斯派：参数是随机变量，假定参数服从一个先验分布，基于观测到的数据来计算参数的后验分布

参数$\theta_c$对D中第c类样本的似然：$P(D_c|\theta_c)=\Pi_{x\in D_c}P(x|\theta_c)$，寻找能最大化似然的参数值

对数似然可防止下溢：$LL(\theta_c)=\log P(D_c|\theta_c)=\log \sum_{x\in D_c}P(x|\theta_c),\hat\theta_c=\arg\max_{\theta_c}LL(\theta_c)$

### 朴素贝叶斯分类器

假设样本的每个属性对分类结果独立地发生影响 - 朴素的原因

$h_{nb}(x)=\arg\max_{c\in Y}P(c)\sum^d_{i=1}P(x_i|c)$

“拉普拉斯平滑修正”用于防止未出现的属性值导致连乘后为0：$P(c)=\frac{|D_c|+1}{|D|+N},P(x_i|c)=\frac{|D_{x_i,c}|+1}{|D_c|+N_i}$，Ni表示第i个属性可能的取值数。拉普拉斯平滑修正是特殊的贝叶斯估计。

### 贝叶斯网（信念网）

$B=<G,\Theta>$。假设属性$x_i$在G中父节点集为$\pi_i$，$\theta_{x_i|\pi_i}=P_B(x_i|\pi_i)$，比如西瓜书P157例子

B将联合概率分布定义为$P_B(x_1,x_2,...,x_d)=\Pi^d_{i=1}P(x_i|\pi_i)$

- 同父结构，给定父节点的值，两个子节点条件独立
- 顺序结构，给定中间结点的值，头尾节点条件独立
- V结构，给定子节点的值，两个父节点必不独立；子节点取值完全未知，父节点相互独立

## EM算法

含有隐变量的概率模型参数的极大似然估计法/极大后验概率估计法。迭代计算参数的估计值，直到收敛。不同的初始值选择得到不同的参数估计值。每步迭代求的是似然函数$L(\theta)=\log P(Y|\theta)$的极大似然估计。

- 输入：观测变量数据Y，隐变量数据Z，联合分布$P(Y,Z|\theta)$，条件分布$P(Z|Y,\theta)$

- 输出：模型参数$\theta$

- 选择参数的初始值$\theta^{(0)}$

- 在第i+1次迭代的E步，计算
  $$
  Q(\theta,\theta^{(i)})=\sum_ZP(Y,Z|\theta)P(Z|Y,\theta)
  $$

- 

- 重复以上两步，直到收敛

EM算法可以用于生成模型的无监督学习，由联合概率分布P（X，Y）表示，可认为无监督学习训练数据是联合分布产生的数据

## 集成学习

### 个体与集成

同质集成中的个体学习器称作“基学习器”、“基学习算法”

异质集成中的个体学习器称“组件学习器”

个体学习器应该兼具“准确性”和“多样性”

两种集成学习方法

- 个体学习器之间存在强依赖关系、必须串行生成：Boosting
- 不存在强依赖关系、可并行生成：Bagging、随机森林

### Boosting

先从初始训练集训练出一个基学习器，再根据基学习器的表现调整训练样本分布，使先前基学习器做错的训练样本在后续得到更多关注，然后基于调整后的样本分布来训练下一个基学习器，直至学习其数目达到指定的值T，将T个基学习器加权结合 - 代表：Adaboost

Adaboost基于基学习器的线性组合$H(x)=\sum^T_{t=1}\alpha_th_t(x)$来最小化指数损失函数$l_{exp}(H|D)=E_{x-D}[e^{-f(x)D(x)}]$

其中f(x)是真实标签。损失函数对H(x)求偏导=0，解得
$$
H(x)=\frac{1}{2}\ln\frac{P(f(x)=1|x)}{P(f(x)=-1|x)}
$$
因此有
$$
sign(H(x))=\arg\max_{y\in \{-1,+1\}}P(f(x)=y|x)
$$
所以指数损失函数最小化时，分类错误率也最小化，因此用指数损失函数代替0/1损失函数，因为其有更好的可微性。

Adaboost算法中ht基于分布Dt产生后，该分类器的权重$\alpha_t$应使$\alpha_th_t$最小化指数损失函数
$$
l_{exp}(\alpha_th_t|D_t)=E_{x-D_t}[e^{-f(x)\alpha_th_t(x)}]\\=e^{-\alpha_t}P(f(x)=h_t(x))+e^{\alpha_t}P(f(x)\ne h_t(x))\\=e^{-\alpha_t}(1-\epsilon_t)+e^{\alpha_t}\epsilon_t
$$
$l_{exp}(\alpha_th_t|D_t)$对$\alpha_t$求导=0，解得$\alpha_t=\frac{1}{2}\ln(\frac{1-\epsilon_t}{\epsilon_t})$

理想的$h_t$可以纠正$H_{t-1}$的全部错误，即最小化：
$$
l_{exp}(H_{t-1}+h_t|D)=E_{x-D}[e^{-f(x)H_{t-1}(x)}e^{-f(x)h_t(x)}]
$$
泰勒级数展开最后一部分得到（注意$f^2(x)=h^2(x)=1$）
$$
l_{exp}(H_{t-1}+h_t|D)=E_{x-D}[e^{-f(x)H_{t-1}(x)}(1-f(x)h_t(x)+\frac{1}{2})]
$$
于是有
$$
h_t(x)=\arg\min_hl_{exp}(H_{t-1}+h_t|D)\\=\arg\max_hE_{x-D}[e^{-f(x)H_{t-1}(x)}f(x)h(x)]\\=\arg\max_hE_{x-D}[\frac{e^{-f(x)H_{t-1}(x)}}{E_{x-D}[e^{-f(x)H_{t-1}(x)}]}f(x)h(x)]
$$
令$D_t$表示分布$D(x)\frac{e^{-f(x)H_{t-1}(x)}}{E_{x-D}[e^{-f(x)H_{t-1}(x)}]}$

等价于令$h_t(x)=\arg\max_hE_{x-D}[\frac{e^{-f(x)H_{t-1}(x)}}{E_{x-D}[e^{-f(x)H_{t-1}(x)}]}f(x)h(x)]=\arg\min_h E_{x-D_t}[\mathbb I(f(x)\ne h(x))]$

上式表明，理想的$h_t$将在分布$D_t$下最小化分类误差。

t+=1，有
$$
D_{t+1}(x)=\frac{D(x)e^{-f(x)H_t(x)}}{E_{x-D}[e^{-f(x)H_t(x)}]}\\=\frac{D(x)e^{-f(x)H_{t-1}(x)}e^{-f(x)\alpha_th_{t}(x)}}{E_{x-D}[e^{-f(x)H_t(x)}]}\\=D_t(x)e^{-f(x)\alpha_th_t(x)}\frac{E_{x-D}[e^{-f(x)H_{t-1}(x)}]}{E_{x-D}[e^{-f(x)H_t(x)}]}
$$
这就是AdaBoost算法的分布更新公式：
$$
w_{t+1,i}=w_{t,i}\frac{e^{-\alpha_tf(x_i)h_t(x_i)}}{\sum_jw_{t,j}e^{-\alpha_tf(x_j)h_t(x_j)}}
$$
由此可知，被错误分类的样本（$f(x)\ne h(x)$）的权值被扩大，被正确分类的样本的权值被缩小；最终的结果由T个基本分类器加权表决，所有$\alpha_t$的和并不为1。

##### AdaBoost算法总结

![adaboost](../../img\adaboost.png)

Boosting算法有2种方式使基学习器对特定的数据分布进行学习：

- 根据样本分布为样本赋予新权重
- 根据样本分补重采样

Boosting主要关注降低**偏差**，因此可以基于弱泛化性能的学习器构建出很强的集成

### Bagging

给定包含m个样本的数据集，又放回地随机采样m个样本 - 原始数据集里63.2%的数据出现在采样数据中（有的出现了多次），有的从未出现。

采样出T个包含m个样本的采样集，训练T个基学习器并结合进行预测

相比于AdaBoost只能用于二分类任务，Bagging效率高、可用于多分类和回归

自助采样的特性 - 采样数据包含约63.2%的训练数据 - 使剩下的36.8%的样本可用作验证集来进行“包外估计”

### 随机森林

在bagging的基础上，在决策树的训练过程中引入随机属性选择：基决策树每个节点的属性选择范围k是全部属性d的一个子集，一般$k=\log_2d$

随机森林的强大性能不仅来自于样本扰动，还来自于属性扰动，使得个体学习器之间的差异度进一步增加

### 结合策略

#### 加权平均法

用于数值预测任务，且权重要非负

#### 投票法

- 绝对多数投票法：若某标记得票过半数，预测为该标记 - 保证可靠性
- 相对多数投票法：预测为得票最多的标记 - 保证提供结果
- 加权投票法：每个分类器乘以权重

硬投票和软投票不能混合

#### 学习法 - Stacking

训练一个新的学习器，输入T个初级学习器的预测，输出一个最终预测标记

将初级学习器的输出概率作为次级学习器的输入属性，用多响应线性回归作为次级学习算法是较好的选择

### 多样性

误差-分歧分解：个体学习器准确性越高、多样性越大，集成越好

多样性度量 - 考虑个体分类器的两两相似性 - 方法：（m个样本，hi和hj的分类结果：abcd分别表示11、1-1、-11、-1-1）

- 不合度量：$dis_{ij}=\frac{b+c}{m}$
- 相关系数：$\rho_{ij}=\frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}$
- Q统计量：$Q_{ij}=\frac{ad-bc}{ad+bc}$
- k统计量：$k=\frac{p_1+p_2}{1-p_2}$
  - $p_1=\frac{a+d}{m},p_2=\frac{(a+b)(a+c)+(c+d)(b+d)}{m^2}$

多样性增强方法：

- 数据样本扰动，对神经网络很有效，但对线性学习器、SVM、朴素贝叶斯不起作用，因为他们是稳定基学习器
- 输入属性扰动：随机子空间算法
- 输出表示扰动
- 算法参数扰动

## 聚类

将样本集划分为k个不相交的簇$\{C_l|l=1,2,...,k\}$，用$\lambda_j$表示样本$x_j$的簇标记

### 性能度量

外部指标：将聚类结果与某个参考模型进行比较；内部指标：直接考察聚类结果

常用外部指标：具体见西瓜书P198-P199

- Jaccard系数
- FM指数
- RI指数

常用内部指标：西瓜书P199

- DB指数
- Dunn指数

### 距离计算

闵可夫斯基距离：$dist(x_i,x_j)=(\sum^n_{u=1}|x_{iu}-x_{ju}|^p)^\frac{1}{p}$，可用于有序属性

- p=1为曼哈顿距离；p=2为欧氏距离

对于无序属性可采用VDM。$m_{u,a}$表示在属性u上取值为a的样本数，$m_{u,a,i}$表示在第i个样本簇中属性u上取值为a的样本数，k为样本簇数，则
$$
VDM_P(a,b)=\sum^k_{i=1}|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}|^p
$$

### 原型聚类

#### k均值算法

最小化平方误差
$$
E=\sum^k_{i=1}\sum_{x\in C_i}||x-\mu_i||^2
$$
采用贪心策略，通过迭代优化来近似求解上式。

西瓜书P203k均值算法：4-8行划分样本，9-16行更新均值向量

![kmeans](/Users/yuwang/Desktop/学习笔记/Study-scripts/img/kmeans.png)

K均值聚类一些特点：类别数k事先指定；以欧氏距离平方表示样本之间的距离；以中心或样本均值表示类别；算法迭代进行，不能保证得到全局最优

#### 学习向量量化（LVQ）

LVQ假设样本带有类别标记，利用监督信息来辅助聚类。LVQ学习一组n维原型向量$\{p_1,p_2,...,p_q\}$，每个原型向量代表一个聚类簇。先对原型向量初始化。在每一轮迭代中，算法随机选取一个有标记的训练样本，找出与其距离最近的原型向量，并根据两者类别标记是否一致来更新原型向量。

更新方式：若对于样本$x_j$，最近的原型向量$p_{i^*}$与$x_j$的类别标记相同，则令$p_{i^*}$向$x_j$的方向靠拢
$$
p'=p_{i^*}+\eta(x_j-p_{i^*})
$$
若不同，令更新后的原型向量远离此样本
$$
p'=p_{i^*}-\eta(x_j-p_{i^*})
$$
由此对样本空间进行了q簇划分

#### 高斯混合聚类

n维样本空间中的随机向量$x$服从高斯分布，则概率密度函数为$p(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$

高斯混合分布$p_M(x)=\sum^k_{i=1}\alpha_ip(x|\mu_i,\Sigma_i)$

根据被混合的概率密度函数进行采样得到训练集$D=\{x_1,x_2,...,x_m\}$，令随机变量$z_j$表示生成样本$x_j$的高斯混合成份，则$P(z_j=i)$对应于$\alpha_i$，则$P(z_j=i|x_j)$给出了样本$x_j$由第i个高斯混合成份生成的后验概率，记为$\gamma_{ji}$

当混合分布已知时，高斯混合聚类将把样本集D划分为k个簇，每个样本的簇标记：$\lambda_j=\arg\max_{i\in \{1,2,...,k\}}\gamma_{ji}$

即簇划分由原型对应后验概率决定

采用极大似然估计求解混合分布的参数，最大化对数似然
$$
LL(D)=\ln(\sum^m_{j=1}p_M(x_j))\\=\sum^m_{j=1}\ln(\sum^k_{i=1}\alpha_ip(x_j|\mu_i,\Sigma_i))
$$
求偏导=0，得到
$$
\mu_i=\frac{\sum^m_{j=1}\gamma_{ji}x_j}{\sum^m_{j=1}\gamma_{ij}}
$$
即将混合成份的后验概率作为权重加权求得均值

类似有
$$
\Sigma_i=\frac{\sum^m_{j=1}\gamma_{ij}(x_j-\mu_i)(x_j-\mu_i)^T}{\sum^m_{j=1}\gamma_{ij}}
$$
求$\alpha_i$时还要考虑和为1的约束，通过拉格朗日乘数法求得
$$
\alpha_i=\frac{1}{m}\sum^m_{j=1}\gamma_{ij}
$$
高斯混合模型EM算法：在迭代中先根据当前参数计算每个样本属于每个混合成份的后验概率，之后根据上面3个式子更新参数。最后根据$\lambda_j=\arg\max_{i\in \{1,2,...,k\}}\gamma_{ji}$决定类别

### 密度聚类

假设聚类结构能通过样本分布的紧密结构确定。

DBSCAN算法的概念

- $\epsilon$-邻域：$x_j$的邻域包含样本集D中与$x_j$距离不大于$\epsilon$的样本
- 核心对象：若$x_j$的邻域包含>=MinPts个样本，则其是一个核心对象
- 密度直达：若$x_i$位于$x_j$的邻域且$x_j$是核心对象，则$x_i$由xj密度直达
- 密度可达：若样本序列$p_1,...,p_n$中$p_1=x_i,p_n=x_j$且$p_{i+1}$由$p_i$密度直达，则$x_j$由$x_i$密度可达
- 密度相连：若$x_k$均由$x_i$和$x_j$密度可达，则xi和xj密度相连

DBSCAN对簇的定义：由密度可达关系导出的最大的密度相连样本集合。即，若x为核心对象，由x密度可达的所有样本组成的集合X，是一个簇

DBSCAN算法：西瓜书P213。1-7行寻找核心对象，10-24行找出由任一核心对象密度可达的样本生成聚类簇。类似于深度优先搜索。

### 层次聚类

AGNES：先将每个样本看作一个初始聚类簇，之后每一步找出距离最近的两个类簇合并，不断重复直至达到预设的聚类簇个数。算法：西瓜书P215.

## 降维

### k-近邻

给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本 - 没有训练过程，称为“懒惰学习”

考虑最近邻：k=1，则出错概率：$P(err)=1-\sum_{c\in Y}P(c|x)P(c|z)$

### 低维嵌入

高位情况下，无法保证测试样本任意小的距离范围内存在训练样本，且对距离计算消耗过大

MDS：多维缩放算法。假定m个样本在原始空间的距离矩阵$D\in R^{m*m},dist_{ij}=||x_i-x_j||$，MDS学习一个样本在$d’$维空间的表示$Z\in R^{d'*m}$，且任意两个样本在$d'$维空间中的欧氏距离等于原始空间中的距离

令$B=Z^TZ$，B为降维后样本的内积矩阵，有$dist^2_{ij}=||z_i||^2+||z_j||^2-2z_i^Tz_j=b_{ii}+b_{jj}-2b_{ij}$

令Z被中心化，则B的行之和、列之和均为0

经过推导可得：$b_{ij}=-\frac{1}{2}(dist^2_{ij}-dist^2_{i·}-dist^2_{·j}+dist^2_{··})$，可求取B

对B做特征值分解，$B=V\Lambda V^T$，假设有$d^*$个非零特征值，构成对角矩阵$\Lambda_*$，相应的特征向量矩阵$V_*$，则Z可表达为$Z=\Lambda^{\frac{1}{2}}V^T_*$

MDS算法：西瓜书P229

### PCA

思想：首先对给定的数据规范化为均值为0，方差为1。之后对数据进行正交变换原来由线性相关的**变量**表示的数据，通过正交变换变成由若干个线性无关的**新变量**表示的数据。将新变量依次称为第一主成分、第二主成分。。

直观解释：数据集合中的样本由实数空间（正交坐标系）中的点表示，空间的一个坐标轴表示一个**变量**。对原坐标系中的数据进行主成分分析等价于进行坐标系旋转变换，将数据投影到新坐标系的坐标轴上：新坐标系的第一坐标轴、第二坐标轴分别表示第一主成分、第二主成分，数据在每一坐标轴上坐标值的平方表示相应变量的方差。这个坐标系是所有可能新坐标系中，坐标轴上方差的和最大的 - 见统计学习方法P298图16.1。坐标轴上方差最大意味着样本点距离坐标轴的距离最小。

考虑由m维随机变量$x=(x_1,...,x_m)^T$到m维随机变量$y=(y_1,...,y_m)^T$的线性变换：
$$
y_i=\alpha^T_ix=\alpha_{i1}x_1+\alpha_{i2}x_2+...+\alpha_{mi}x_m\\
$$
总体主成分的定义：对于上面的线性变换，如果他们满足下列条件：

- 系数向量是单位向量$\alpha^T_i\alpha_i=1$
- 变量$y_i$与$y_j$互不相关: $ cov(y_i,y_j)=0$
- 变量$y_1$是x的所有线性变换中方差最大的，$y_2$是与$y_1$不相关的x的所有线性变换中方差最大的。。。y1是x的第一主成分。。。

定理：设$x$是m维随机变量，$\Sigma$是$x$的协方差矩阵，$\Sigma$的特征值分别是$\lambda_1>=\lambda_2>=\lambda_3>=...=0$，特征值对应的单位特征向量分别是$\alpha_1,\alpha_2,...,\alpha_m$，则x的第k主成分是$y_k=\alpha_k^Tx$，第k主成分对应的方差是$var(y_k)=\alpha^T_k\Sigma \alpha_k=\lambda_k$

当取前k大特征值对应的特征向量时，能最大限度地保留原变量方差的信息

注意：计算样本协方差矩阵的方法：
$$
x_j=(x_{1j},x_{2j},...,x_{mj}),j=1,...,n\\
S=[s_{ij}]_{m\times m},s_{ij}=\frac{1}{n-1}\sum^n_{k=1}(x_{ik}-\hat x_i)(x_{ik}-\hat x_j)\\
\hat x_i=\frac{1}{n}\sum^n_{k=1}x_{ik}
$$

## 特征工程

### 归一化

#### 方式

- 线性函数归一化
- 零均值归一化 - 正态分布

#### 作用

通过梯度下降算法学习的机器学习模型（线性回归、逻辑回归、SVM、NN），归一化参数之后参数可以更加一致速度地更新，从而找到最优解

但对于决策树没有作用，因为归一化不影响特征的信息增益

### 类别型特征

- 序号编码 - 有顺序的
- one-hot - 使用向量的稀疏表示
- 二进制编码 - 节省空间

### 文本表示模型

#### TFIDF

TF：单词t在文档d中出现的频率；$IDF(t)=\log\frac{|D|}{|d,t\in d|+1}$

#### N-gram

略

#### LDA

见“概率图模型”

## 概率图模型

### 隐马尔可夫模型

推断：利用已知变量O推测未知变量Y的分布$P(Y|O)$

- 生成式：考虑联合分布$P(Y,R,O)$
- 判别式：考虑条件分布$P(Y,R|O)$

概率图模型：用图来表达变量相关关系的概率模型，一个节点表示一个或一组随机变量

- 有向无环图：贝叶斯网
- 无向图：马尔科夫网

隐马尔科夫模型是最简单的动态贝叶斯网：状态变量（不可观测，取值空间为N）和观测变量，t时刻的观测变量仅取决于当前的状态变量，当前的状态变量仅取决于前一时刻的状态变量
$$
P(x_1,y_1,...,x_n,y_n)=P(y_1)P(x_1|y_1)\sum^n_{i=2}P(y_{i}|y_{i-1})P(x_i|y_i)
$$
此外隐马尔科夫模型还包括

- 状态转移概率，通常记为矩阵A：$a_{ij}=P(y_{t+1}=s_j|y_t=s_i)$
- 输出观测概率，通常记为矩阵B：$b_{ij}=P(x_t=o_j|y_t=s_i)$
- 初始状态概率，模型在初始时刻各状态i出现的概率：$\pi=(\pi_1,\pi_2,...,\pi_N),\pi_i=P(y_1=s_i)$

关注的问题

- 概率计算问题：给定模型$\lambda=[A,B,\pi]$，如何计算产生观测序列$x=\{x_1,...,x_n\}$的概率$P(x|\lambda)$
- 预测问题：给定模型和观测序列，如何找到与此观测序列最匹配的状态序列$y=\{y_1,...,y_n\}$
- 学习问题：给定观测序列，如何训练模型使其最好地描述观测数据？即最大化$P(x|\lambda)$

#### 概率计算算法

前向算法：前向概率：到t时刻部分观测序列为$o_1,o_2,...,o_t$且状态为$q_i$的概率
$$
\alpha_t(i)=P(o_1,o_2,...,o_t,i_t=q_i|\lambda)
$$

- 输入：隐马尔科夫模型$\lambda$，观测序列$O$

- 输出：观测序列概率$P(O|\lambda)$

- 初值：$\alpha_1(i)=\pi_ib_i(o_1)$

- 递推：对t=1，2，...，T-1，
  $$
  \alpha_{t+1}(i)=[\sum^N_{j=1}\alpha_t(j)a_{ji}]b_i(o_{t+1})
  $$

- 终止：$P(O|\lambda)=\sum^N_{i=1}\alpha_T(i)$

递推公式中，$\alpha_t(j)a_{ji}$表示t时刻状态为$q_j$且下一时刻转变为$q_i$的概率，将t时刻所有可能的状态求和，得到了t+1时刻状态为qi的概率，$b_i(o_{t+1})$表示在t+1时刻观测到$o_{1,2,...,t+1}$且状态处于$q_i$。最后对所有可能产生观测序列O的终止状态$q_i$求和。

后向算法：后向概率：$\beta_t(i)=P(o_{t+1},o_{t+2},...,o_T|i_t=q_i,\lambda)$

- 输入：隐马尔科夫模型$\lambda$，观测序列$O$

- 输出：观测序列概率$P(O|\lambda)$

- 对t=T-1,T-2,..., 1,
  $$
  \beta_t(i)=\sum^N_{j=1}a_{ij}b_j(o_{t+1})\beta_{t+1}(j)
  $$

- $P(O|\lambda)=\sum^N_{i=1}\pi_ib_i(o_1)\beta_1(i)$

#### 学习算法

监督学习：训练数据包括观测序列和对应的状态序列

设$A_{ij}$：样本中t时刻处于状态i，t+1时刻处于状态j的频数，有
$$
\hat a_{ij}=\frac{A_{ij}}{\sum^N_{j=1}A_{ij}}
$$
设$B_{jk}$：状态为j并且观测为k的频数，有
$$
\hat b_J(k)=\frac{B_{jk}}{\sum^M_{k=1}B_{jk}}
$$
无监督学习：没有状态序列，将状态序列看作不可观测的隐数据I：$P(O|\lambda)=\sum_IP(O|I,\lambda)P(I|\lambda)$

由EM算法实现

#### 预测算法

- 近似算法
- 维特比算法

### 马尔科夫随机场

是一种概率无向图模型：满足成对、局部或全局马尔科夫性的联合概率分布P(X)，最大特点是易于因子分解

典型的马尔可夫网

- 团：任意两节点之间都有边连接的子集
- 极大团：在团中加入另外任何一个节点都不再形成团
  - 每个节点至少出现在一个极大团中

多个变量之间的联合概率分布能基于团分解为多个因子的乘积，每个因子仅与一个团相关。对于n个变量$X=\{x_1,...,x_n\}$，团$Q\in C$对应的变量集合$X_Q$：
$$
P(X)=\frac{1}{Z}\Pi_{Q\in C}\phi_Q(X_Q)\\
Z=\sum_{X}\Pi_{Q\in C}\phi_Q(X_Q)
$$
$\phi_Q$为与团Q对应的势函数。Z是对X所有可能的取值求和。

因为团$Q$必被一个极大团$Q*$所包含，可以从重写为
$$
P(X)=\frac{1}{Z}\Pi_{Q\in C^*}\phi_Q(X_Q)\\
Z=\sum_{X}\Pi_{Q\in C^*}\phi_Q(X_Q)
$$
这样可以减少乘积项的个数

”全局马尔可夫性“：给定两个变量子集$X_A$、$X_B$的分离集$X_C$，则这两个变量子集条件独立：$X_A\perp X_B|X_C$，即$P(X_A,X_B|X_C)=P(X_A|X_C)P(X_B|X_C)$

由此得到两个重要推论：

- 局部马尔科夫性：给定某变量v的邻接变量W，则此变量条件独立于其他变量O
  $$
  P(X_v,X_O|X_W)=P(X_v|X_W)P(X_O|X_W)
  $$

- 成对马尔可夫性：给定所有其他变量，2个非邻接变量条件独立
  $$
  P(X_u,X_v|X_O)=P(X_u|X_O)P(X_v|X_O)
  $$


势函数经常包含指数函数：
$$
\phi_Q(X_Q)=e^{-H_Q(X_Q)}
$$
H(X)是定义在变量X_Q上的实值函数，常见形式为:
$$
H_Q(X_Q)=\sum_{u,v\in Q,u\ne v}\alpha_{uv}x_ux_v+\sum_{v\in Q}\beta_vx_v
$$

### 条件随机场

是在给定随机变量X的条件下，随机变量Y的马尔科夫随机场

隐马尔科夫模型和马尔科夫随机场都是生成式模型，CRF是判别式，给定观测序列X，构建条件标记序列的概率模型$P(Y|X)$。$Y_v$表示与节点v对应的随机变量，$n(v)$表示节点v的邻接节点，若图中每个节点都满足马尔科夫性
$$
P(Y_v|X,Y_{V-v})=P(Y_v|X,Y_{n(v)})
$$
则$P(Y|X) $构成一个条件随机场。序列标注问题中一般考虑线性链条件随机场，最大团是相邻两个节点的集合。

使用势函数和团来将条件概率因子分解为
$$
P(y|x)=\frac{1}{Z}\exp(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i))
$$
其中$t_k(y_{i-1},y_i,x,i)$是定义在观测序列的两个相邻标记位置上的转移特征函数；$s_l(y_i,x,i)$是定义在观测序列的标记位置i上的状态特征函数 - 需要定义合适的特征函数，刻画数据的一些可能成立的经验特性，通常取值为1或0 - CRF完全由特征函数和对应的权值确定。Z是规范化因子，类似于马尔科夫随机场，在所有可能的输出序列上进行。

与马尔科夫随机场都是用团上的势函数来定义概率，但条件随机场处理条件概率。

### 话题模型(LDA)

生成式有向图模型，$w_{t,n}$表示文档t中词n的词频（共T个w向量），$\beta_{k,n}$表示话题k中词n的词频（共K个beta向量）

LDA从生成式模型的角度来看待文档和话题。向量$\Theta_t$表示文档t中包含每个话题的比例，进而由话题”生成“文档：

- 根据参数为$\alpha$的迪利克雷分布随机采样一个话题分布$\Theta_t$
- 生成文档中的N个词
  - 根据$\Theta_t$得到文档t中词n的话题$z_{t,n}$
  - 根据指派的话题所对应的词频分布$\beta_k$随机采样生成词

文档中的词频$w_{t,n}$是唯一的已观测变量，它依赖于对这个词进行的话题指派$z_{t,n}$以及话题所对应的此频$\beta_k$，$z_{t,n}$对应于话题分布$\Theta_t$，$\Theta_t$对应于迪利克雷分布参数$\alpha$；话题对应的词频依赖于参数$\eta$，因此：
$$
p(W,z,\beta,\Theta|\alpha,\eta)=\Pi^T_{t=1}p(\Theta_t|\alpha)\Pi^K_{k=1}p(\beta_k|\eta)(\Pi^N_{n=1}P(w_{t,n}|z_{t,n}，\beta_k)P(z_{t,n}|\Theta_t))
$$
其中$p(\Theta_t|\alpha),p(\beta_k|\eta)$分别是以$\alpha,\eta$为参数的K维和N维迪利克雷分布

LDA通过给定的训练数据极大似然估计参数
$$
LL(\alpha,\eta)=\sum^T_{t=1}\ln p(w_t|\alpha,\eta)
$$
确定参数后，可推断文档集所对应的话题结构
$$
p(z,\beta,\Theta|W,\alpha,\eta)=\frac{p(W,z,\beta,\Theta)}{p(W|\alpha,\eta)}
$$

## 综合知识点

### 监督学习模型总结

![Supervised ML Summary](../img/Supervised ML Summary.png)

直接学习条件概率分布$P(Y|X)$或决策函数$Y=f(X)$的方法称为判别式方法，包括感知机、k近邻、决策树、逻辑回归与最大熵模型、支持向量机、提升方法、条件随机场

先学习联合分布$P(X,Y)$进而求得条件概率分布$P(Y|X)$的方法称为生成式方法，包括朴素贝叶斯和隐马尔科夫模型

